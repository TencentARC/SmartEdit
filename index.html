<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>SmartEdit</title>
<link href="./files/style.css" rel="stylesheet">
<script type="text/javascript" src="./files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./files/jquery.js"></script>
</head>


<!--从这里开始改.../-->
<!--https://yuzhou914.github.io/SmartEdit/-->
<body>
<div class="content">
  <div class="logo" style="text-align: center;">
    <a href="index.html">
      <img src="./assets/Logo.jpg">
    </a>
  </div>

  <h1><strong>SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models</strong></h1>
  <p id="authors">
    <a href="https://openreview.net/profile?id=~Yuzhou_Huang1">Yuzhou Huang</a><sup>1,2*</sup>
    <a href="https://liangbinxie.github.io/">Liangbin Xie</a><sup>2,3,5*</sup>
    <a href="https://xinntao.github.io/">Xintao Wang</a><sup>2,4&#x2709;</sup>
    <a href="https://github.com/jiangyzy">Ziyang Yuan</a><sup>2,8</sup>
    <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a><sup>4</sup>
    <a href="https://geyixiao.com/">Yixiao Ge</a><sup>2,4</sup>
    <a href="https://www.fst.um.edu.mo/personal/jtzhou/">Jiantao Zhou</a><sup>3</sup>
    <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN">Chao Dong</a><sup>5,7</sup>
    <a href="https://sse.cuhk.edu.cn/en/faculty/huangrui">Rui Huang</a><sup>6</sup>
    <a href="http://zhangruimao.site/">Ruimao Zhang</a><sup>1&#x2709;</sup>
    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en/">Ying Shan</a><sup>2,4</sup><br>
    <br>

  <span style="font-size: 18px">
    <sup>1</sup>School of Data Science, Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, China &nbsp;&nbsp;&nbsp;&nbsp;
    <sup>2</sup>ARC Lab, Tencent PCG &nbsp;&nbsp;
    <sup>3</sup>University of Macau &nbsp;&nbsp;
    <sup>4</sup>Tencent AI Lab &nbsp;&nbsp;
    <sup>5</sup>Shenzhen Institute of Advanced Technology
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>6</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>7</sup>Shanghai Artificial Intelligence Laboratory &nbsp;&nbsp;&nbsp;&nbsp;
    <sup>8</sup>Tsinghua University</span></p>

  <div style="text-align: center;">
    <span style="font-size: 14px">
      <sup>*</sup> Equal Contribution  &nbsp;&nbsp;&nbsp;&nbsp;
      &#x2709; Corresponding Authors</span>
  </div>

  <br>
  <br>
  <h3 style="text-align:center">
  <em>
    SmartEdit aims at handling complex understanding (the instructions that contain various object attributes like
    location, relative size, color, and in or outside the mirror) and reasoning scenarios.
  </em></h3>
  <img src="./assets/1-teaser.jpg" class="teaser-gif" style="width:100%;"><br>

  <font size="+2">
          <p style="text-align: center;">
            <!--<a href="https://arxiv.org/abs/2312.04461" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;-->
            <a href="https://arxiv.org/abs/2312.06739" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/TencentARC/SmartEdit" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="files/demo.txt" target="_blank">[Demo]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font>
    <p style="text-align: center;">
    </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>Abstract</strong></h2>
  <p>
    Current instruction-based editing methods, such as InstructPix2Pix, often fail to produce satisfactory results in complex scenarios due to their dependence on the simple CLIP text encoder in diffusion models.
    To rectify this, this paper introduces <strong>SmartEdit</strong>, a novel approach to instruction-based image editing that leverages Large Language Models (LLMs) with visual inputs to enhance their understanding and reasoning capabilities.
    However, direct integration of these elements still faces challenges in situations requiring complex reasoning.
    To mitigate this, we propose a Bidirectional Interaction Module that enables comprehensive bidirectional information interactions between the input image and the MLLM output.
    During training, we initially incorporate perception data to boost the perception and understanding capabilities of diffusion models. Subsequently, we demonstrate that a small amount of complex instruction editing data can effectively stimulate SmartEdit's editing capabilities for more complex instructions.
    We further construct a new evaluation dataset, Reason-Edit, specifically tailored for complex instruction-based image editing.
    Both quantitative and qualitative results on this evaluation dataset indicate that our SmartEdit surpasses previous methods, paving the way for the practical application of complex instruction-based image editing.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>SmartEdit Framework</strong></h2>
  <img class="summary-img" src="./assets/2-SmartEdit.jpg" style="width:100%;">
  <p>
    The overall framework of SmartEdit.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>SmartEdit on Understanding Scenarios</strong></h2>
  <img class="summary-img" src="./assets/3-Understanding.jpg" style="width:100%;">
  <p>
    Visual effects of SmartEdit on complex understanding scenarios.
    It can be seen that for complex understanding scenarios,
    SmartEdit has good instruction-based editing effects.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>SmartEdit on Reasoning Scenarios</strong></h2>
  <img class="summary-img" src="./assets/4-Reasoning.jpg" style="width:100%;">
  <p>
    Visual effects of SmartEdit on complex reasoning scenarios.
    It can be seen that for complex reasoning scenarios,
    SmartEdit has good instruction-based editing effects.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>SmartEdit on MagicBrush TestSet</strong></h2>
  <img class="summary-img" src="./assets/5-MagicBrush.jpg" style="width:100%;">
  <p>
    The performance of SmartEdit on the MagicBrush test dataset.
    SmartEdit has good editing effects on the MagicBrush test dataset,
    not only for single-turn but also for multi-turn.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>Comparisons on Understanding Scenarios</strong></h2>
  <img class="summary-img" src="./assets/SuppUnderstanding1.jpg" style="width:100%;">
  <p>
    Qualitative comparison on complex understanding scenarios.
    Compared to other methods,
    SmartEdit can precisely edit specific objects in images according to instructions,
    while keeping the content in other areas unchanged.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>Comparisons on Reasoning Scenarios</strong></h2>
  <img class="summary-img" src="./assets/SuppReasoning2.jpg" style="width:100%;">
  <p>
    Qualitative comparison on reasoning scenarios.
    For reasoning scenarios,
    SmartEdit can effectively utilize the reasoning capabilities of the LLM to identify the corresponding objects,
    and then edit the objects according to the instructions.
    Other methods perform poorly in these scenarios.
  </p>
</div>


<div class="content">
  <h2 style="text-align:center;"><strong>Comparisons on MagicBrush TestSet</strong></h2>
  <img class="summary-img" src="./assets/SuppMagicBrush3.jpg" style="width:100%;">
  <p>
    Qualitative comparison between our SmartEdit, MagicBrush-168, MagicBrush-52, InstructDiffusion, and InstructPix2Pix.
    Compared against other methods, SmartEdit effectively adheres to the instructions, showcasing superior results.
  </p>
</div>


<div class="content">
  <h2>BibTex</h2>
  <code> @article{huang2023smartedit,<br>
  &nbsp;&nbsp;title={SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models},<br>
  &nbsp;&nbsp;author={Huang, Yuzhou and Xie, Liangbin and Wang, Xintao and Yuan, Ziyang and Cun, Xiaodong and Ge, Yixiao and Zhou, Jiantao and Dong, Chao and Huang, Rui and Zhang, Ruimao and Shan, Ying},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2312.06739},<br>
  &nbsp;&nbsp;year={2023}<br>
  }
  </code>


</div>
<div class="content" id="acknowledgements">
  <p>
    Our project page is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  </p>
</div>

</body>
</html>
